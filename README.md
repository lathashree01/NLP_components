# About This Repository

This repository contains implementations of various core components used in NLP from scratch (Inspired by many amazing resources).

## Implemented Components

### Attention Mechanisms
- Self-Attention: Computes attention scores between all pairs of tokens in a sequence, allowing the model to capture contextual relationships.
- Multi-Head Attention: Extends self-attention by projecting inputs into multiple heads, enabling the model to learn different types of dependencies in parallel.
- Masked Multi-Head Attention: A variant of multi-head attention used in autoregressive tasks, where future tokens are masked to prevent information leakage during training.
