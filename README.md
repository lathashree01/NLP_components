# About This Repository

This repository contains implementations of various core components used in NLP from scratch (Inspired by many amazing resources).

## Implemented Components

### Attention Mechanisms ./attention_mechanism
Attention mechanisms are a powerful technique in deep learning that allow models to focus on specific parts of the input data when making predictions. They have been widely used in various applications, including natural language processing and computer vision.

### You can find implementations of the following attention mechanisms:
- Self-Attention
- Multi-Head Attention
- Masked Multi-Head Attention
